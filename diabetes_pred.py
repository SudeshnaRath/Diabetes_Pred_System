# -*- coding: utf-8 -*-
"""Final Major Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oyxkeYceIhOEURVETfZMxvsA3dx7AAMP
"""

!pip install regressormetricgraphplot
!pip install scipy

!pip install catboost

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

#  loading the diabetes dataset to a pandas DataFrame
diabetes_dataset = pd.read_csv('/content/diabetes_prediction_dataset.csv')

# number of rows and Columns in this dataset
diabetes_dataset.shape

# getting the statistical measures of the data
diabetes_dataset.describe()

for col in diabetes_dataset.select_dtypes(include='object').columns:
    print(f"{col}: {diabetes_dataset[col].unique()}")

#  loading the diabetes dataset to a pandas DataFrame
import pandas as pd  # Import the pandas module

diabetes_dataset = pd.read_csv('/content/diabetes_prediction_dataset.csv')

# ... (other code) ...

del diabetes_dataset['smoking_history']
del diabetes_dataset['gender']

# Save the DataFrame to a CSV file using to_csv
diabetes_dataset.to_csv('diabetes_dataset_modified.csv', index=False) # call the to_csv method on the dataframe and save to file

mod_data= pd.read_csv('/content/diabetes_dataset_modified.csv')

print(mod_data.isnull().sum())

mod_data['diabetes'].value_counts()
# 1 indicating the presence of diabetes and 0 inducating not diabetes.

mod_data.nunique()

mod_data.groupby('diabetes').mean(numeric_only=True)

sns.countplot(x='age', data=mod_data)
plt.show()

sns.countplot(x='hypertension', data=mod_data)
plt.show()

sns.countplot(x='HbA1c_level', data=mod_data)
plt.show()

sns.countplot(x='bmi', data=mod_data)
plt.show()

sns.countplot(x='blood_glucose_level', data=mod_data)
plt.show()

sns.countplot(x='heart_disease', data=mod_data)
plt.show()

sns.pairplot(mod_data)
plt.show()

"""SPLITTING DATA"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np
import pandas as pd
import re
from sklearn.metrics import (
    accuracy_score, roc_auc_score, precision_score, recall_score,
    f1_score, confusion_matrix, roc_curve, auc
)
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score
)
from scipy.stats import pearsonr
from regressormetricgraphplot import CompareModels
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.naive_bayes import GaussianNB
from math import sqrt

# Separate features and target
# Replace 'Target' with the actual name of your target column
X = mod_data.drop('diabetes', axis=1)  # Features
y = mod_data['diabetes']               # Target

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

"""KNN"""

# Initialize and train the KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
# Make predictions
y_pred_knn = knn.predict(X_test)
y_prob_knn = knn.predict_proba(X_test)[:, 1]  # Predicted probabilities for ROC-AUC

# Calculate evaluation metrics
acc_knn = accuracy_score(y_test, y_pred_knn)
prec_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
roc_auc_knn = roc_auc_score(y_test, y_prob_knn)

print(f"Accuracy: {acc_knn:.2f}")
print(f"Precision: {prec_knn:.2f}")
print(f"Recall: {recall_knn:.2f}")
print(f"F1 Score: {f1_knn:.2f}")
print(f"ROC-AUC: {roc_auc_knn:.2f}")


# Generate a classification report
class_report = classification_report(y_test, y_pred_knn)
print("\nClassification Report:")
print(class_report)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_knn)
plt.figure(figsize=(8, 6))
plt.title('Confusion Matrix', fontsize=16)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='coolwarm', xticklabels=['Non-Diabetic', 'Diabetic'], yticklabels=['Non-Diabetic', 'Diabetic'])
plt.xlabel('Predicted', fontsize=14)
plt.ylabel('Actual', fontsize=14)
plt.show()

# Calculate metrics
# 1. R (Pearson Coefficient)
r_knn, _ = pearsonr(y_test, y_pred_knn)
# 2. R² Score
r2_knn = r2_score(y_test, y_pred_knn)

# 3. Root Mean Squared Error (RMSE)
rmse_knn= np.sqrt(mean_squared_error(y_test, y_pred_knn))

# 4. Mean Absolute Error (MAE)
mae_knn= mean_absolute_error(y_test, y_pred_knn)

# Print the metrics
print(f"R (Pearson Coefficient): {r_knn:.2f}")
print(f"R² Score: {r2_knn:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse_knn:.2f}")
print(f"Mean Absolute Error (MAE): {mae_knn:.2f}")

# Visualization of Metrics
metrics = ['R (Pearson)', 'R² Score', 'RMSE', 'MAE']
values = [r_knn, r2_knn, rmse_knn, mae_knn]

plt.figure(figsize=(10, 6))
sns.barplot(x=metrics, y=values, palette='coolwarm')
plt.title('Performance Metrics', fontsize=16)
plt.ylabel('Score', fontsize=14)
plt.xlabel('Metrics', fontsize=14)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
for i, v in enumerate(values):
    plt.text(i, v , f"{v:.2f}", ha='center', fontsize=12)
plt.show()

# Comparative Metrics Table
metrics_table = pd.DataFrame({
    "Metric": ["Pearson R", "R2 Score", "MSE", "RMSE", "Accuracy", "Recall", "Precision", "F1 Score"],
    "KNN": [r_knn, r2_knn, mae_knn, rmse_knn, acc_knn, recall_knn, prec_knn, f1_knn]
})

print(metrics_table)

# Function to plot ROC-AUC curves
def plot_roc_auc(models):
    plt.figure(figsize=(10, 8))
    for model_name, model in models:
        # Fit the model
        model.fit(X_train, y_train)
        y_prob = model.predict_proba(X_test)[:, 1]  # Probability scores

        # Compute ROC curve and AUC
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc = roc_auc_score(y_test, y_prob)

        # Plot ROC curve
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')

    # Add plot details
    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
    plt.title('ROC-AUC Curve Comparison', fontsize=16)
    plt.xlabel('False Positive Rate (FPR)', fontsize=14)
    plt.ylabel('True Positive Rate (TPR)', fontsize=14)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

# Initialize models
models = [
    ("KNN", KNeighborsClassifier(n_neighbors=5)),
]

# Plot ROC-AUC curves for all models
plot_roc_auc(models)

"""Decision tree"""

# Train Decision Tree Classifier
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions
y_pred_dt = dt_model.predict(X_test)
y_prob_dt = dt_model.predict_proba(X_test)[:, 1]  # Probabilities for ROC-AUC

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_dt)

# Visualize Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Diabetic', 'Diabetic'],
            yticklabels=['Non-Diabetic', 'Diabetic'])
plt.title('Confusion Matrix', fontsize=16)
plt.xlabel('Predicted', fontsize=14)
plt.ylabel('Actual', fontsize=14)
plt.show()

from sklearn.metrics import accuracy_score
# Calculate evaluation metrics
accuracy_dt= accuracy_score(y_test, y_pred_dt)
prec_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)
roc_auc_dt = roc_auc_score(y_test, y_prob_dt)

# Print evaluation metrics
print(f"Accuracy: {accuracy_dt:.2f}")
print(f"Precision: {prec_dt:.2f}")
print(f"Recall: {recall_dt:.2f}")
print(f"F1 Score: {f1_dt:.2f}")
print(f"ROC-AUC: {roc_auc_dt:.2f}")
# Calculate metrics
# 1. R (Pearson Coefficient)
r_dt, _ = pearsonr(y_test, y_pred_dt)

# 2. R² Score
r2_dt = r2_score(y_test, y_pred_dt)

# 3.-dt Root Mean Squared Error (RMSE)
rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))

# 4. Mean Absolute Error (MAE)
mae_dt = mean_absolute_error(y_test, y_pred_dt)

# Print the metrics
print(f"R (Pearson Coefficient): {r_dt:.2f}")
print(f"R² Score: {r2_dt:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse_dt:.2f}")
print(f"Mean Absolute Error (MAE): {mae_dt:.2f}")

# Visualization of Metrics
metrics = ['R (Pearson)', 'R² Score-dt', 'RMSE', 'MAE']
values = [r_dt, r2_dt, rmse_dt, mae_dt]

plt.figure(figsize=(10, 6))
sns.barplot(x=metrics, y=values, palette='flare')
plt.title('Performance Metrics', fontsize=16)
plt.ylabel('Score', fontsize=14)
plt.xlabel('Metrics', fontsize=14)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
for i, v in enumerate(values):
    plt.text(i, v , f"{v:.2f}", ha='center', fontsize=12)
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score

# Function to plot ROC-AUC curves
def plot_roc_auc(models):
    plt.figure(figsize=(10, 8))
    for model_name, model in models:
        # Fit the model
        model.fit(X_train, y_train)
        y_prob = model.predict_proba(X_test)[:, 1]  # Probability scores

        # Compute ROC curve and AUC
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc = roc_auc_score(y_test, y_prob)

        # Plot ROC curve
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')

    # Add plot details
    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
    plt.title('ROC-AUC Curve Comparison', fontsize=16)
    plt.xlabel('False Positive Rate (FPR)', fontsize=14)
    plt.ylabel('True Positive Rate (TPR)', fontsize=14)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

# Initialize models
models = [
    ("KNN", KNeighborsClassifier(n_neighbors=5)),
    ("Decision Tree", DecisionTreeClassifier(random_state=4))
]

# Plot ROC-AUC curves for all models
plot_roc_auc(models)

# Comparative Metrics Table
metrics_table = pd.DataFrame({
    "Metric": ["Pearson R", "R2 Score", "MSE", "RMSE", "Accuracy", "Recall", "Precision", "F1 Score"],
    "KNN": [r_knn, r2_knn, mae_knn, rmse_knn, acc_knn, recall_knn, prec_knn, f1_knn],
    "Decision Tree": [r_dt, r2_dt, mae_dt, rmse_dt, accuracy_dt, recall_dt, prec_dt, f1_dt]
})

print(metrics_table)

"""NAIVE BAYES"""

from sklearn.naive_bayes import GaussianNB
from math import sqrt
# Initialize the Naive Bayes classifier
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Predict and calculate probabilities
y_pred_nb = nb_model.predict(X_test)
y_prob_nb = nb_model.predict_proba(X_test)[:, 1]

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_nb)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Diabetic', 'Diabetic'],
            yticklabels=['Non-Diabetic', 'Diabetic'])
plt.title('Confusion Matrix', fontsize=16)
plt.xlabel('Predicted', fontsize=14)
plt.ylabel('Actual', fontsize=14)
plt.show()

from sklearn.metrics import accuracy_score
# Calculate evaluation metrics
accuracy_nb= accuracy_score(y_test, y_pred_nb)
prec_nb = precision_score(y_test, y_pred_nb)
recall_nb = recall_score(y_test, y_pred_nb)
f1_nb = f1_score(y_test, y_pred_nb)
roc_auc_nb = roc_auc_score(y_test, y_prob_nb)

# Print evaluation metrics
print(f"Accuracy: {accuracy_nb:.2f}")
print(f"Precision: {prec_nb:.2f}")
print(f"Recall: {recall_nb:.2f}")
print(f"F1 Score: {f1_nb:.2f}")
print(f"ROC-AUC: {roc_auc_nb:.2f}")
# Calculate metrics
# 1. R (Pearson Coefficient)
r_nb, _ = pearsonr(y_test, y_pred_nb)

# 2. R² Score
r2_nb = r2_score(y_test, y_pred_nb)

# 3.-dt Root Mean Squared Error (RMSE)
rmse_nb = np.sqrt(mean_squared_error(y_test, y_pred_nb))

# 4. Mean Absolute Error (MAE)
mae_nb = mean_absolute_error(y_test, y_pred_nb)

# Print the metrics
print(f"R (Pearson Coefficient): {r_nb:.2f}")
print(f"R² Score: {r2_nb:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse_nb:.2f}")
print(f"Mean Absolute Error (MAE): {mae_nb:.2f}")

# Visualization of Metrics
metrics = ['R (Pearson)', 'R² Score-dt', 'RMSE', 'MAE']
values = [r_nb, r2_nb, rmse_nb, mae_nb]

plt.figure(figsize=(10, 6))
sns.barplot(x=metrics, y=values, palette='flare')
plt.title('Performance Metrics', fontsize=16)
plt.ylabel('Score', fontsize=14)
plt.xlabel('Metrics', fontsize=14)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
for i, v in enumerate(values):
    plt.text(i, v , f"{v:.2f}", ha='center', fontsize=12)
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score

# Function to plot ROC-AUC curves
def plot_roc_auc(models):
    plt.figure(figsize=(10, 8))
    for model_name, model in models:
        # Fit the model
        model.fit(X_train, y_train)
        y_prob = model.predict_proba(X_test)[:, 1]  # Probability scores

        # Compute ROC curve and AUC
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc = roc_auc_score(y_test, y_prob)

        # Plot ROC curve
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')

    # Add plot details
    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
    plt.title('ROC-AUC Curve Comparison', fontsize=16)
    plt.xlabel('False Positive Rate (FPR)', fontsize=14)
    plt.ylabel('True Positive Rate (TPR)', fontsize=14)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

# Initialize models
models = [
    ("KNN", KNeighborsClassifier(n_neighbors=5)),
    ("Decision Tree", DecisionTreeClassifier(random_state=4)),
    ("Naive Bayes", GaussianNB())
]

# Plot ROC-AUC curves for all models
plot_roc_auc(models)

# Comparative Metrics Table
metrics_table = pd.DataFrame({
    "Metric": ["Pearson R", "R2 Score", "MSE", "RMSE", "Accuracy", "Recall", "Precision", "F1 Score"],
    "KNN": [r_knn, r2_knn, mae_knn, rmse_knn, acc_knn, recall_knn, prec_knn, f1_knn],
    "Decision Tree": [r_dt, r2_dt, mae_dt, rmse_dt, accuracy_dt, recall_dt, prec_dt, f1_dt],
    "Naive Bayes": [r_nb, r2_nb, mae_nb, rmse_nb, accuracy_nb, recall_nb, prec_nb, f1_nb]
})

print(metrics_table)

"""Random Forest

"""

import pandas as pd
import numpy as np
from math import sqrt
from sklearn.ensemble import RandomForestClassifier

# Train the Random Forest Classifier
rf_model = RandomForestClassifier(random_state=4, n_estimators=300)
rf_model.fit(X_train, y_train)

# Predictions
y_pred_rf = rf_model.predict(X_test)
y_prob_rf = rf_model.predict_proba(X_test)[:, 1]

from sklearn.metrics import accuracy_score
# Calculate evaluation metrics
accuracy_rf= accuracy_score(y_test, y_pred_rf)
prec_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
roc_auc_rf = roc_auc_score(y_test, y_prob_rf)

# Print evaluation metrics
print(f"Accuracy: {accuracy_rf:.2f}")
print(f"Precision: {prec_rf:.2f}")
print(f"Recall: {recall_rf:.2f}")
print(f"F1 Score: {f1_rf:.2f}")
print(f"ROC-AUC: {roc_auc_rf:.2f}")
# Calculate metrics
# 1. R (Pearson Coefficient)
r_rf, _ = pearsonr(y_test, y_pred_rf)

# 2. R² Score
r2_rf = r2_score(y_test, y_pred_rf)

# 3.-dt Root Mean Squared Error (RMSE)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))

# 4. Mean Absolute Error (MAE)
mae_rf = mean_absolute_error(y_test, y_pred_rf)

# Print the metrics
print(f"R (Pearson Coefficient): {r_rf:.2f}")
print(f"R² Score: {r2_rf:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse_rf:.2f}")
print(f"Mean Absolute Error (MAE): {mae_rf:.2f}")

# Visualization of Metrics
metrics = ['R (Pearson)', 'R² Score-dt', 'RMSE', 'MAE']
values = [r_rf, r2_rf, rmse_rf, mae_rf]

plt.figure(figsize=(10, 6))
sns.barplot(x=metrics, y=values, palette='flare')
plt.title('Performance Metrics', fontsize=16)
plt.ylabel('Score', fontsize=14)
plt.xlabel('Metrics', fontsize=14)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
for i, v in enumerate(values):
    plt.text(i, v , f"{v:.2f}", ha='center', fontsize=12)
plt.show()

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Diabetic', 'Diabetic'],
            yticklabels=['Non-Diabetic', 'Diabetic'])
plt.title('Confusion Matrix', fontsize=16)
plt.xlabel('Predicted', fontsize=14)
plt.ylabel('Actual', fontsize=14)
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score

# Function to plot ROC-AUC curves
def plot_roc_auc(models):
    plt.figure(figsize=(10, 8))
    for model_name, model in models:
        # Fit the model
        model.fit(X_train, y_train)
        y_prob = model.predict_proba(X_test)[:, 1]  # Probability scores

        # Compute ROC curve and AUC
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc = roc_auc_score(y_test, y_prob)

        # Plot ROC curve
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')

    # Add plot details
    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
    plt.title('ROC-AUC Curve Comparison', fontsize=16)
    plt.xlabel('False Positive Rate (FPR)', fontsize=14)
    plt.ylabel('True Positive Rate (TPR)', fontsize=14)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

# Initialize models
models = [
    ("KNN", KNeighborsClassifier(n_neighbors=5)),
    ("Decision Tree", DecisionTreeClassifier(random_state=4)),
    ("Naive Bayes", GaussianNB()),
    ("Random Forest", RandomForestClassifier(random_state=4, n_estimators=300))
]

# Plot ROC-AUC curves for all models
plot_roc_auc(models)

# Comparative Metrics Table
metrics_table = pd.DataFrame({
    "Metric": ["Pearson R", "R2 Score", "MSE", "RMSE", "Accuracy", "Recall", "Precision", "F1 Score"],
    "KNN": [r_knn, r2_knn, mae_knn, rmse_knn, acc_knn, recall_knn, prec_knn, f1_knn],
    "Decision Tree": [r_dt, r2_dt, mae_dt, rmse_dt, accuracy_dt, recall_dt, prec_dt, f1_dt],
    "Naive Bayes": [r_nb, r2_nb, mae_nb, rmse_nb, accuracy_nb, recall_nb, prec_nb, f1_nb],
    "Random Forest": [r_rf, r2_rf, mae_rf, rmse_rf, accuracy_rf, recall_rf, prec_rf, f1_rf]
})

print(metrics_table)

"""XG Boost"""

from xgboost import XGBClassifier

# Train the XGBoost Classifier
xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred_xg = xgb_model.predict(X_test)
y_prob_xg = xgb_model.predict_proba(X_test)[:, 1]  # Probabilities for ROC-AUC

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_xg)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='inferno',
            xticklabels=['Non-Diabetic', 'Diabetic'],
            yticklabels=['Non-Diabetic', 'Diabetic'])
plt.title('Confusion Matrix', fontsize=16)
plt.xlabel('Predicted', fontsize=14)
plt.ylabel('Actual', fontsize=14)
plt.show()

from sklearn.metrics import accuracy_score
# Calculate evaluation metrics
accuracy_xg= accuracy_score(y_test, y_pred_xg)
prec_xg = precision_score(y_test, y_pred_xg)
recall_xg = recall_score(y_test, y_pred_xg)
f1_xg = f1_score(y_test, y_pred_xg)
roc_auc_xg = roc_auc_score(y_test, y_prob_xg)

# Print evaluation metrics
print(f"Accuracy: {accuracy_xg:.2f}")
print(f"Precision: {prec_xg:.2f}")
print(f"Recall: {recall_xg:.2f}")
print(f"F1 Score: {f1_xg:.2f}")
print(f"ROC-AUC: {roc_auc_xg:.2f}")
# Calculate metrics
# 1. R (Pearson Coefficient)
r_xg, _ = pearsonr(y_test, y_pred_xg)

# 2. R² Score
r2_xg = r2_score(y_test, y_pred_xg)

# 3.-dt Root Mean Squared Error (RMSE)
rmse_xg = np.sqrt(mean_squared_error(y_test, y_pred_xg))

# 4. Mean Absolute Error (MAE)
mae_xg = mean_absolute_error(y_test, y_pred_xg)

# Print the metrics
print(f"R (Pearson Coefficient): {r_xg:.2f}")
print(f"R² Score: {r2_xg:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse_xg:.2f}")
print(f"Mean Absolute Error (MAE): {mae_xg:.2f}")

# Visualization of Metrics
metrics = ['R (Pearson)', 'R² Score-dt', 'RMSE', 'MAE']
values = [r_xg, r2_xg, rmse_xg, mae_xg]

plt.figure(figsize=(10, 6))
sns.barplot(x=metrics, y=values, palette='flare')
plt.title('Performance Metrics', fontsize=16)
plt.ylabel('Score', fontsize=14)
plt.xlabel('Metrics', fontsize=14)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
for i, v in enumerate(values):
    plt.text(i, v , f"{v:.2f}", ha='center', fontsize=12)
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score

# Function to plot ROC-AUC curves
def plot_roc_auc(models):
    plt.figure(figsize=(10, 8))
    for model_name, model in models:
        # Fit the model
        model.fit(X_train, y_train)
        y_prob = model.predict_proba(X_test)[:, 1]  # Probability scores

        # Compute ROC curve and AUC
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc = roc_auc_score(y_test, y_prob)

        # Plot ROC curve
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')

    # Add plot details
    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
    plt.title('ROC-AUC Curve Comparison', fontsize=16)
    plt.xlabel('False Positive Rate (FPR)', fontsize=14)
    plt.ylabel('True Positive Rate (TPR)', fontsize=14)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

# Initialize models
models = [
    ("KNN", KNeighborsClassifier(n_neighbors=5)),
    ("Decision Tree", DecisionTreeClassifier(random_state=4)),
    ("Naive Bayes", GaussianNB()),
    ("Random Forest", RandomForestClassifier(random_state=4, n_estimators=300)),
    ("XGBoost", XGBClassifier(random_state=42, eval_metric='logloss'))
]

# Plot ROC-AUC curves for all models
plot_roc_auc(models)

# Comparative Metrics Table
metrics_table = pd.DataFrame({
    "Metric": ["Pearson R", "R2 Score", "MSE", "RMSE", "Accuracy", "Recall", "Precision", "F1 Score"],
    "KNN": [r_knn, r2_knn, mae_knn, rmse_knn, acc_knn, recall_knn, prec_knn, f1_knn],
    "Decision Tree": [r_dt, r2_dt, mae_dt, rmse_dt, accuracy_dt, recall_dt, prec_dt, f1_dt],
    "Naive Bayes": [r_nb, r2_nb, mae_nb, rmse_nb, accuracy_nb, recall_nb, prec_nb, f1_nb],
    "Random Forest": [r_rf, r2_rf, mae_rf, rmse_rf, accuracy_rf, recall_rf, prec_rf, f1_rf],
    "XGBoost": [r_xg, r2_xg, mae_xg, rmse_xg, accuracy_xg, recall_xg, prec_xg, f1_xg]
})

print(metrics_table)

"""RIDGE CLASSIFIER"""

from sklearn.linear_model import RidgeClassifier

# Initialize Ridge Classifier
ridge_model = RidgeClassifier()

# Train the model
ridge_model.fit(X_train, y_train)

# Make predictions
y_pred_rc = ridge_model.predict(X_test)

# RidgeClassifier does not have predict_proba,
# so we use decision_function to get a score
# and then scale it to a probability-like value using sigmoid
from scipy.special import expit
y_prob_rc = expit(ridge_model.decision_function(X_test))

# Evaluation metrics
accuracy_rc = accuracy_score(y_test, y_pred_rc)
prec_rc = precision_score(y_test, y_pred_rc)
recall_rc = recall_score(y_test, y_pred_rc)
f1_rc = f1_score(y_test, y_pred_rc)
mse_rc = mean_squared_error(y_test, y_pred_rc)
rmse_rc = sqrt(mse_rc)
r2_rc = r2_score(y_test, y_pred_rc)
r_rc = np.corrcoef(y_test, y_pred_rc)[0, 1]

# Print evaluation metrics
print(f"R (Pearson Coefficient): {r_rc:.2f}")
print(f"R2 Score: {r2_rc:.2f}")
print(f"Mean Squared Error: {mse_rc:.2f}")
print(f"Root Mean Squared Error: {rmse_rc:.2f}")
print(f"Accuracy: {accuracy_rc:.2f}")
print(f"Precision: {prec_rc:.2f}")
print(f"Recall: {recall_rc:.2f}")
print(f"F1 Score: {f1_rc:.2f}")

# Visualization of Metrics
metrics = ['R (Pearson)', 'R² Score-dt', 'RMSE', 'MAE']
values = [r_rc, r2_rc, rmse_rc, mse_rc]

plt.figure(figsize=(10, 6))
sns.barplot(x=metrics, y=values, palette='viridis')
plt.title('Performance Metrics', fontsize=16)
plt.ylabel('Score', fontsize=14)
plt.xlabel('Metrics', fontsize=14)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
for i, v in enumerate(values):
    plt.text(i, v , f"{v:.2f}", ha='center', fontsize=12)
plt.show()

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_rc)

# Visualize Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis',
            xticklabels=['Non-Diabetic', 'Diabetic'],
            yticklabels=['Non-Diabetic', 'Diabetic'])
plt.title('Confusion Matrix - Ridge Classifier', fontsize=16)
plt.xlabel('Predicted', fontsize=14)
plt.ylabel('Actual', fontsize=14)
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score

# Function to plot ROC-AUC curves
def plot_roc_auc(models):
    plt.figure(figsize=(10, 8))
    for model_name, model in models:
        # Fit the model
        model.fit(X_train, y_train)
       # Use decision_function for RidgeClassifier and apply sigmoid
        if model_name == "Ridge Classifier":
            y_prob = expit(model.decision_function(X_test))
        else:
            y_prob = model.predict_proba(X_test)[:, 1]  # Probability scores for other models

        # Compute ROC curve and AUC
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc = roc_auc_score(y_test, y_prob)

        # Plot ROC curve
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')

    # Add plot details
    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
    plt.title('ROC-AUC Curve Comparison', fontsize=16)
    plt.xlabel('False Positive Rate (FPR)', fontsize=14)
    plt.ylabel('True Positive Rate (TPR)', fontsize=14)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

# Initialize models
models = [
    ("KNN", KNeighborsClassifier(n_neighbors=5)),
    ("Decision Tree", DecisionTreeClassifier(random_state=4)),
    ("Naive Bayes", GaussianNB()),
    ("Random Forest", RandomForestClassifier(random_state=4, n_estimators=300)),
    ("XGBoost", XGBClassifier(random_state=42, eval_metric='logloss')),
    ("Ridge Classifier", RidgeClassifier())
]

# Plot ROC-AUC curves for all models
plot_roc_auc(models)

# Comparative Metrics Table
metrics_table = pd.DataFrame({
    "Metric": ["Pearson R", "R2 Score", "MSE", "RMSE", "Accuracy", "Recall", "Precision", "F1 Score"],
    "KNN": [r_knn, r2_knn, mae_knn, rmse_knn, acc_knn, recall_knn, prec_knn, f1_knn],
    "Decision Tree": [r_dt, r2_dt, mae_dt, rmse_dt, accuracy_dt, recall_dt, prec_dt, f1_dt],
    "Naive Bayes": [r_nb, r2_nb, mae_nb, rmse_nb, accuracy_nb, recall_nb, prec_nb, f1_nb],
    "Random Forest": [r_rf, r2_rf, mae_rf, rmse_rf, accuracy_rf, recall_rf, prec_rf, f1_rf],
    "XGBoost": [r_xg, r2_xg, mae_xg, rmse_xg, accuracy_xg, recall_xg, prec_xg, f1_xg],
    "Ridge Classifier": [r_rc, r2_rc, mse_rc, rmse_rc, accuracy_rc, recall_rc, prec_rc, f1_rc]
})

print(metrics_table)

"""CATBoost"""

import catboost as cb
from catboost import CatBoostClassifier

# Train CatBoost Classifier
catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.05, depth=6, random_seed=42, cat_features=[], verbose=200)
catboost_model.fit(X_train, y_train)

# Predictions
y_pred_cat = catboost_model.predict(X_test)
y_prob_cat = catboost_model.predict_proba(X_test)[:, 1]  # Probability scores for ROC-AUC

# Evaluation Metrics
accuracy_cat= accuracy_score(y_test, y_pred_cat)
precision_cat = precision_score(y_test, y_pred_cat)
recall_cat = recall_score(y_test, y_pred_cat)
f1_cat = f1_score(y_test, y_pred_cat)
roc_auc_cat = roc_auc_score(y_test, y_prob_cat)
r2_cat = r2_score(y_test, y_pred_cat)
rmse_cat = sqrt(mean_squared_error(y_test, y_pred_cat))
mse_cat = mean_squared_error(y_test, y_pred_cat)

# Pearson's correlation (R)
r_cat = np.corrcoef(y_test, y_pred_cat)[0, 1]

# Print the evaluation metrics
print(f"R (Pearson's correlation): {r_cat:.2f}")
print(f"R²: {r2_cat:.2f}")
print(f"RMSE: {rmse_cat:.2f}")
print(f"MSE: {mse_cat:.2f}")
print(f"Accuracy: {accuracy_cat:.2f}")
print(f"Precision: {precision_cat:.2f}")
print(f"Recall: {recall_cat:.2f}")
print(f"F1 Score: {f1_cat:.2f}")
print(f"ROC-AUC: {roc_auc_cat:.2f}")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_cat)

# Visualize Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis', xticklabels=['Non-Diabetic', 'Diabetic'], yticklabels=['Non-Diabetic', 'Diabetic'])
plt.title('Confusion Matrix - CatBoost', fontsize=16)
plt.xlabel('Predicted', fontsize=14)
plt.ylabel('Actual', fontsize=14)
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score

# Function to plot ROC-AUC curves
def plot_roc_auc(models):
    plt.figure(figsize=(10, 8))
    for model_name, model in models:
        # Fit the model
        model.fit(X_train, y_train)
       # Use decision_function for RidgeClassifier and apply sigmoid
        if model_name == "Ridge Classifier":
            y_prob = expit(model.decision_function(X_test))
        else:
            y_prob = model.predict_proba(X_test)[:, 1]  # Probability scores for other models

        # Compute ROC curve and AUC
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc = roc_auc_score(y_test, y_prob)

        # Plot ROC curve
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')

    # Add plot details
    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
    plt.title('ROC-AUC Curve Comparison', fontsize=16)
    plt.xlabel('False Positive Rate (FPR)', fontsize=14)
    plt.ylabel('True Positive Rate (TPR)', fontsize=14)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

# Initialize models
models = [
    ("KNN", KNeighborsClassifier(n_neighbors=5)),
    ("Decision Tree", DecisionTreeClassifier(random_state=4)),
    ("Naive Bayes", GaussianNB()),
    ("Random Forest", RandomForestClassifier(random_state=4, n_estimators=300)),
    ("XGBoost", XGBClassifier(random_state=42, eval_metric='logloss')),
    ("Ridge Classifier", RidgeClassifier()),
    ("CatBoost", CatBoostClassifier(iterations=1000, learning_rate=0.05, depth=6, random_seed=42, cat_features=[], verbose=200))
]

# Plot ROC-AUC curves for all models
plot_roc_auc(models)

# Comparative Metrics Table
metrics_table = pd.DataFrame({
    "Metric": ["Pearson R", "R2 Score", "MSE", "RMSE", "Accuracy", "Recall", "Precision", "F1 Score"],
    "KNN": [r_knn, r2_knn, mae_knn, rmse_knn, acc_knn, recall_knn, prec_knn, f1_knn],
    "Decision Tree": [r_dt, r2_dt, mae_dt, rmse_dt, accuracy_dt, recall_dt, prec_dt, f1_dt],
    "Naive Bayes": [r_nb, r2_nb, mae_nb, rmse_nb, accuracy_nb, recall_nb, prec_nb, f1_nb],
    "Random Forest": [r_rf, r2_rf, mae_rf, rmse_rf, accuracy_rf, recall_rf, prec_rf, f1_rf],
    "XGBoost": [r_xg, r2_xg, mae_xg, rmse_xg, accuracy_xg, recall_xg, prec_xg, f1_xg],
    "Ridge Classifier": [r_rc, r2_rc, mse_rc, rmse_rc, accuracy_rc, recall_rc, prec_rc, f1_rc],
    "CatBoost": [r_cat, r2_cat, mse_cat, rmse_cat, accuracy_cat, recall_cat, precision_cat, f1_cat]
})

print(metrics_table)

"""SVM"""

from sklearn.svm import SVC
# Train SVM model
svm_model = SVC(probability=True, kernel='linear', random_state=3)
svm_model.fit(X_train, y_train)

# Predictions and probabilities
y_pred_svm = svm_model.predict(X_test)
y_prob_svm = svm_model.predict_proba(X_test)[:, 1]

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_svm)

# Visualize Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Diabetic', 'Diabetic'],
            yticklabels=['Non-Diabetic', 'Diabetic'])
plt.title('Confusion Matrix', fontsize=16)
plt.xlabel('Predicted', fontsize=14)
plt.ylabel('Actual', fontsize=14)
plt.show()

# Define Pearson correlation function
def pearson_coeff(y_test, y_pred):
    return np.corrcoef(y_test, y_pred_svm)[0, 1]

# Root Mean Squared Error
def root_mean_squared_error(y_test, y_pred):
    return sqrt(mean_squared_error(y_test, y_pred_svm))

# Function to plot ROC-AUC curves for models
def plot_roc_auc(models, X_test, y_test):
    plt.figure(figsize=(10, 8))
    for model_name, model in models:
        y_prob = model.decision_function(X_test) if hasattr(model, "decision_function") else model.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_prob_svm)
        auc = roc_auc_score(y_test, y_prob_svm)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')
    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
    plt.title('ROC-AUC Curve Comparison', fontsize=16)
    plt.xlabel('False Positive Rate (FPR)', fontsize=14)
    plt.ylabel('True Positive Rate (TPR)', fontsize=14)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

from sklearn.metrics import roc_curve, roc_auc_score

# Function to plot ROC-AUC curves
def plot_roc_auc(models):
    plt.figure(figsize=(10, 8))
    for model_name, model in models:
        # Fit the model
        model.fit(X_train, y_train)
        y_prob = model.predict_proba(X_test)[:, 1]  # Probability scores

        # Compute ROC curve and AUC
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc = roc_auc_score(y_test, y_prob)

        # Plot ROC curve
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')

    # Add plot details
    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
    plt.title('ROC-AUC Curve Comparison', fontsize=16)
    plt.xlabel('False Positive Rate (FPR)', fontsize=14)
    plt.ylabel('True Positive Rate (TPR)', fontsize=14)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

# Initialize models
models = []
models = [
    ("KNN", KNeighborsClassifier(n_neighbors=5)),
    ("Decision Tree", DecisionTreeClassifier(random_state=4)),
    ("SVM", svm_model)
]

# Plot ROC-AUC curves for all models
plot_roc_auc(models)

# Comparative table of metrics
def comparative_table(metrics):
    df = pd.DataFrame(metrics)
    display(df)
    return df


# Evaluation metrics
metrics = {
    "Model": ["SVM"],
    "Pearson (R)": [pearson_coeff(y_test, y_pred_svm)],
    "R2 Score": [r2_score(y_test, y_pred_svm)],
    "MSE": [mean_squared_error(y_test, y_pred_svm)],
    "RMSE": [root_mean_squared_error(y_test, y_pred_svm)],
    "Accuracy": [accuracy_score(y_test, y_pred_svm)],
    "Precision": [precision_score(y_test, y_pred_svm)],
    "Recall": [recall_score(y_test, y_pred_svm)],
    "F1 Score": [f1_score(y_test, y_pred_svm)]
}

# Print evaluation metrics
print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred_svm)}")
print(f"Metrics:\n{pd.DataFrame(metrics)}")

# Confusion Matrix Visualization
conf_matrix = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Diabetic', 'Diabetic'],
            yticklabels=['Non-Diabetic', 'Diabetic'])
plt.title('Confusion Matrix', fontsize=16)
plt.xlabel('Predicted', fontsize=14)
plt.ylabel('Actual', fontsize=14)
plt.show()

# Comparative Table
metrics_table = comparative_table(metrics)